{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Rolling Window Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following notebook showcases an example workflow of creating rolling window features and buidling a model to predict which customers will buy in next 4 weeks.\n",
    "\n",
    "This uses dummy sales data but the idea can be implemented on actual sales data and can also be expanded to include other available data sources such as clickstream data, call centre data, email contacts data, etc.\n",
    "\n",
    "***\n",
    "\n",
    "<b>Spark 3.1.2</b> (with Python 3.8) has been used for this notebook.<br>\n",
    "Refer to [spark documentation](https://spark.apache.org/docs/3.1.2/api/sql/index.html) for help with <b>data ops functions</b>.<br>\n",
    "Refer to [this article](https://medium.com/analytics-vidhya/installing-and-using-pyspark-on-windows-machine-59c2d64af76e) to <b>install and use PySpark on Windows machine</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a spark session\n",
    "To create a SparkSession, use the following builder pattern:\n",
    " \n",
    "`spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"Word Count\")\\\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "    .getOrCreate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:27:37.285231Z",
     "start_time": "2021-12-27T15:27:37.222144Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:55:59.457116Z",
     "start_time": "2021-12-27T15:55:58.622469Z"
    }
   },
   "outputs": [],
   "source": [
    "#initiating spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:56:01.516747Z",
     "start_time": "2021-12-27T15:56:00.848965Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"rolling_window\")\\\n",
    "    .config(\"spark.executor.memory\", \"1536m\")\\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:56:02.827635Z",
     "start_time": "2021-12-27T15:56:02.784777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://C-5CG9074HT0.hbc.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://127.0.0.1:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>rolling_window</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23322bdae88>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep\n",
    "\n",
    "We will be using window functions to compute the relative features for all dates. We will first aggregate the data to customer x week level so it is easier to handle.\n",
    "\n",
    "All the required dimension tables have to be joined with the sales table prior to the aggregation so that we can create all the features required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:56:13.466426Z",
     "start_time": "2021-12-27T15:56:05.116827Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:57:08.611897Z",
     "start_time": "2021-12-27T15:56:15.381365Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sales = spark.read.csv('./data/rw_sales.csv',inferSchema=True,header=True)\n",
    "df_customer = spark.read.csv('./data/clustering_customer.csv',inferSchema=True,header=True)\n",
    "df_product = spark.read.csv('./data/clustering_product.csv',inferSchema=True,header=True)\n",
    "df_payment = spark.read.csv('./data/clustering_payment.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Quick exploration of the datasets:</b>\n",
    "1. We have sales data that captures date, customer id, product, quantity, dollar amount & payment type at order x item level. `order_item_id` refers to each unique product in each order\n",
    "2. We have corresponding dimension tables for customer info, product info, and payment tender info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:04:45.420570Z",
     "start_time": "2021-12-27T16:04:44.606624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|order_id|order_item_id|   tran_dt|customer_id|dollars|qty|product_id|payment_type_id|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|       1|            1|2020-01-01|        572|    550|  1|        20|              2|\n",
      "|       2|            2|2020-01-01|        532|    630|  3|        11|              2|\n",
      "|       3|            3|2020-01-01|        608|    450|  2|        18|              4|\n",
      "|       4|            4|2020-01-01|        424|    110|  2|        10|              2|\n",
      "|       5|            5|2020-01-01|        584|    250|  1|         8|              4|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:05:29.939101Z",
     "start_time": "2021-12-27T16:04:48.467214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 20000, 19622)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# order_item_id is the primary key\n",
    "(df_sales.count(),\n",
    " df_sales.selectExpr('count(Distinct order_item_id)').collect()[0][0],\n",
    " df_sales.selectExpr('count(Distinct order_id)').collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:06:47.209586Z",
     "start_time": "2021-12-27T16:06:47.172191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- tran_dt: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- dollars: integer (nullable = true)\n",
      " |-- qty: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- payment_type_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:06:49.192660Z",
     "start_time": "2021-12-27T16:06:49.136160Z"
    }
   },
   "outputs": [],
   "source": [
    "# fix date type for tran_dt\n",
    "df_sales = df_sales.withColumn('tran_dt', F.to_date('tran_dt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:06:51.070807Z",
     "start_time": "2021-12-27T16:06:50.643193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+---------+------------+----------------+\n",
      "|customer_id|age|hh_income|omni_shopper|email_subscribed|\n",
      "+-----------+---+---------+------------+----------------+\n",
      "|          1| 46|   640000|           0|               0|\n",
      "|          2| 32|   890000|           1|               1|\n",
      "|          3| 45|   772000|           0|               0|\n",
      "|          4| 46|   303000|           0|               1|\n",
      "|          5| 38|   412000|           0|               0|\n",
      "+-----------+---+---------+------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customer.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:07:11.010038Z",
     "start_time": "2021-12-27T16:06:52.135922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have 1k unique customers in sales data with all their info in customer dimension table\n",
    "(df_sales.selectExpr('count(Distinct customer_id)').collect()[0][0],\n",
    " df_customer.count(),\n",
    " df_customer.selectExpr('count(Distinct customer_id)').collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:07:13.669443Z",
     "start_time": "2021-12-27T16:07:12.300819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+\n",
      "|product_id|category|price|\n",
      "+----------+--------+-----+\n",
      "|         1|       A|  450|\n",
      "|         2|       B|   80|\n",
      "|         3|       C|  250|\n",
      "|         4|       D|  400|\n",
      "|         5|       E|   50|\n",
      "+----------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# product dimension table provides category and price for each product\n",
    "df_product.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:07:22.013724Z",
     "start_time": "2021-12-27T16:07:15.944047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 22)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_product.count(),\n",
    " df_product.selectExpr('count(Distinct product_id)').collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:07:23.954989Z",
     "start_time": "2021-12-27T16:07:23.527717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "|payment_type_id|payment_type|\n",
      "+---------------+------------+\n",
      "|              1|        cash|\n",
      "|              2| credit card|\n",
      "|              3|  debit card|\n",
      "|              4|   gift card|\n",
      "|              5|      others|\n",
      "+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# payment type table maps the payment type id from sales table\n",
    "df_payment.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join all dim tables and add week_end column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:07:26.316106Z",
     "start_time": "2021-12-27T16:07:26.207076Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sales = df_sales.join(df_product.select('product_id','category'), on=['product_id'], how='left')\n",
    "df_sales = df_sales.join(df_payment, on=['payment_type_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>week_end column: Saturday of every week</b>\n",
    "\n",
    "`dayofweek()` returns 1-7 correspondng to Sun-Sat for a date.\n",
    "\n",
    "Using this, we will convert each date to the date corresponding to the Saturday of that week (week: Sun-Sat) using below logic:<br/>\n",
    "`date + 7 - dayofweek()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:07:28.483110Z",
     "start_time": "2021-12-27T16:07:28.471020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_type_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- tran_dt: date (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- dollars: integer (nullable = true)\n",
      " |-- qty: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:07:32.339244Z",
     "start_time": "2021-12-27T16:07:32.279767Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sales = df_sales.withColumn('week_end',\n",
    "    F.col('tran_dt') + 7 - F.dayofweek('tran_dt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:07:34.810845Z",
     "start_time": "2021-12-27T16:07:33.663356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------+-------------+----------+-----------+-------+---+--------+------------+----------+\n",
      "|payment_type_id|product_id|order_id|order_item_id|   tran_dt|customer_id|dollars|qty|category|payment_type|  week_end|\n",
      "+---------------+----------+--------+-------------+----------+-----------+-------+---+--------+------------+----------+\n",
      "|              2|        20|       1|            1|2020-01-01|        572|    550|  1|       D| credit card|2020-01-04|\n",
      "|              2|        11|       2|            2|2020-01-01|        532|    630|  3|       A| credit card|2020-01-04|\n",
      "|              4|        18|       3|            3|2020-01-01|        608|    450|  2|       C|   gift card|2020-01-04|\n",
      "|              2|        10|       4|            4|2020-01-01|        424|    110|  2|       E| credit card|2020-01-04|\n",
      "|              4|         8|       5|            5|2020-01-01|        584|    250|  1|       C|   gift card|2020-01-04|\n",
      "+---------------+----------+--------+-------------+----------+-----------+-------+---+--------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customer_id x week_end aggregation\n",
    "We will creating following features at weekly level. These will then be aggregated for multiple time frames using window functions for the final dataset.\n",
    "1. Sales\n",
    "2. No. of orders\n",
    "3. No. of units\n",
    "4. Sales split by category\n",
    "5. Sales split by payment type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:07:36.918907Z",
     "start_time": "2021-12-27T16:07:36.820503Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sales_agg = df_sales.groupBy('customer_id','week_end').agg(\n",
    "    F.sum('dollars').alias('sales'),\n",
    "    F.countDistinct('order_id').alias('orders'),\n",
    "    F.sum('qty').alias('units'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:07:43.262568Z",
     "start_time": "2021-12-27T16:07:38.098758Z"
    }
   },
   "outputs": [],
   "source": [
    "# category split pivot\n",
    "df_sales_cat_agg = df_sales.withColumn('category', F.concat(F.lit('cat_'), F.col('category')))\n",
    "\n",
    "df_sales_cat_agg = df_sales_cat_agg.groupBy('customer_id','week_end').pivot('category').agg(F.sum('dollars'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:07:50.067270Z",
     "start_time": "2021-12-27T16:07:44.273669Z"
    }
   },
   "outputs": [],
   "source": [
    "# payment type split pivot\n",
    "# clean-up values in payment type column\n",
    "df_payment_agg = df_sales.withColumn(\n",
    "    'payment_type',\n",
    "    F.concat(F.lit('pay_'), F.regexp_replace(F.col('payment_type'),' ','_')))\n",
    "\n",
    "df_payment_agg = df_payment_agg.groupby('customer_id','week_end').pivot('payment_type').agg(F.max('dollars'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:07:51.151896Z",
     "start_time": "2021-12-27T16:07:50.894584Z"
    }
   },
   "outputs": [],
   "source": [
    "# join all together\n",
    "df_sales_agg = df_sales_agg.join(df_sales_cat_agg, on=['customer_id','week_end'], how='left')\n",
    "df_sales_agg = df_sales_agg.join(df_payment_agg,   on=['customer_id','week_end'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:09:44.064452Z",
     "start_time": "2021-12-27T16:07:52.363178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17488"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_agg = df_sales_agg.persist()\n",
    "df_sales_agg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:09:48.656173Z",
     "start_time": "2021-12-27T16:09:47.888018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+------+-----+-----+-----+-----+-----+-----+--------+---------------+--------------+-------------+----------+\n",
      "|customer_id|  week_end|sales|orders|units|cat_A|cat_B|cat_C|cat_D|cat_E|pay_cash|pay_credit_card|pay_debit_card|pay_gift_card|pay_others|\n",
      "+-----------+----------+-----+------+-----+-----+-----+-----+-----+-----+--------+---------------+--------------+-------------+----------+\n",
      "|         67|2019-10-05| 2300|     2|    5| null| null| null| 2300| null|    null|           1200|          null|         null|      null|\n",
      "|         80|2020-01-11|  900|     1|    2|  900| null| null| null| null|    null|            900|          null|         null|      null|\n",
      "|         81|2020-08-01|  450|     1|    3| null|  450| null| null| null|    null|            450|          null|         null|      null|\n",
      "|         86|2020-02-08|  550|     1|    1|  550| null| null| null| null|    null|            550|          null|         null|      null|\n",
      "|         88|2019-06-01|  740|     2|    2|  450| null|  290| null| null|    null|            450|          null|         null|      null|\n",
      "+-----------+----------+-----+------+-----+-----+-----+-----+-----+-----+--------+---------------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_agg.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Missing weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:09:57.377013Z",
     "start_time": "2021-12-27T16:09:57.314425Z"
    }
   },
   "outputs": [],
   "source": [
    "# cust level min and max weeks\n",
    "df_cust = df_sales_agg.groupBy('customer_id').agg(\n",
    "    F.min('week_end').alias('min_week'),\n",
    "    F.max('week_end').alias('max_week'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:10:02.295454Z",
     "start_time": "2021-12-27T16:10:02.281810Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to get a dataframe with 1 row per date in provided range\n",
    "def pandas_date_range(start, end):\n",
    "    dt_rng = pd.date_range(start=start, end=end, freq='W-SAT') # W-SAT required as we want all Saturdays\n",
    "    df_date = pd.DataFrame(dt_rng, columns=['date'])\n",
    "    return df_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:10:38.129900Z",
     "start_time": "2021-12-27T16:10:06.686107Z"
    }
   },
   "outputs": [],
   "source": [
    "# use the cust level table and create a df with all Saturdays in our range\n",
    "date_list = df_cust.selectExpr('min(min_week)', 'max(max_week)').collect()[0]\n",
    "min_date = date_list[0]\n",
    "max_date = date_list[1]\n",
    "\n",
    "# use the function and create df\n",
    "df_date_range = spark.createDataFrame(pandas_date_range(min_date, max_date))\n",
    "\n",
    "# date format\n",
    "df_date_range = df_date_range.withColumn('date',F.to_date('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:10:47.420687Z",
     "start_time": "2021-12-27T16:10:41.852051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_date_range = df_date_range.repartition(1).persist()\n",
    "df_date_range.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Cross join the date list df with the cust table to create the filled base table</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:10:53.450451Z",
     "start_time": "2021-12-27T16:10:53.377032Z"
    }
   },
   "outputs": [],
   "source": [
    "df_base = df_cust.crossJoin(F.broadcast(df_date_range))\n",
    "\n",
    "# filter to keep only week_end since first week per customer\n",
    "df_base = df_base.where(F.col('date')>=F.col('min_week'))\n",
    "\n",
    "# rename date to week_end\n",
    "df_base = df_base.withColumnRenamed('date','week_end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Join with the aggregat week level table to create full base table</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:11:29.198886Z",
     "start_time": "2021-12-27T16:11:29.036526Z"
    }
   },
   "outputs": [],
   "source": [
    "df_base = df_base.join(df_sales_agg, on=['customer_id','week_end'], how='left')\n",
    "df_base = df_base.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:12:09.823940Z",
     "start_time": "2021-12-27T16:11:32.705734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95197"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base = df_base.persist()\n",
    "df_base.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:15:38.879011Z",
     "start_time": "2021-12-27T16:15:21.338063Z"
    }
   },
   "outputs": [],
   "source": [
    "# write base table as parquet\n",
    "df_base.repartition(8).write.parquet('./data/rw_base/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:15:47.170833Z",
     "start_time": "2021-12-27T16:15:46.859040Z"
    }
   },
   "outputs": [],
   "source": [
    "df_base = spark.read.parquet('./data/rw_base/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## y-variable\n",
    "\n",
    "Determining whether a customer buys something in the next 4 weeks of current week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:16:00.297007Z",
     "start_time": "2021-12-27T16:16:00.242176Z"
    }
   },
   "outputs": [],
   "source": [
    "# flag 1/0 for weeks with purchases\n",
    "df_base = df_base.withColumn('purchase_flag', F.when(F.col('sales')>0,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:16:05.631769Z",
     "start_time": "2021-12-27T16:16:05.457735Z"
    }
   },
   "outputs": [],
   "source": [
    "# window to aggregate the flag over next 4 weeks\n",
    "df_base = df_base.withColumn(\n",
    "    'purchase_flag_next_4w',\n",
    "    F.max('purchase_flag').over(\n",
    "        Window.partitionBy('customer_id').orderBy('week_end').rowsBetween(1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "We will be aggregating the features columns over various time intervals (1/4/13/26/52 weeks) to create a rich set of lookback features. We will also create derived features post aggregating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:16:36.193420Z",
     "start_time": "2021-12-27T16:16:36.066174Z"
    }
   },
   "outputs": [],
   "source": [
    "# we can create and keep Window() objects that can be referenced in multiple formulas\n",
    "# we don't need a window definition for 1w features as these are already present\n",
    "window_4w  = Window.partitionBy('customer_id').orderBy('week_end').rowsBetween(-3,Window.currentRow)\n",
    "window_13w = Window.partitionBy('customer_id').orderBy('week_end').rowsBetween(-12,Window.currentRow)\n",
    "window_26w = Window.partitionBy('customer_id').orderBy('week_end').rowsBetween(-25,Window.currentRow)\n",
    "window_52w = Window.partitionBy('customer_id').orderBy('week_end').rowsBetween(-51,Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:16:38.608974Z",
     "start_time": "2021-12-27T16:16:38.595162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_id',\n",
       " 'week_end',\n",
       " 'min_week',\n",
       " 'max_week',\n",
       " 'sales',\n",
       " 'orders',\n",
       " 'units',\n",
       " 'cat_A',\n",
       " 'cat_B',\n",
       " 'cat_C',\n",
       " 'cat_D',\n",
       " 'cat_E',\n",
       " 'pay_cash',\n",
       " 'pay_credit_card',\n",
       " 'pay_debit_card',\n",
       " 'pay_gift_card',\n",
       " 'pay_others',\n",
       " 'purchase_flag',\n",
       " 'purchase_flag_next_4w']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Direct features</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:16:55.709340Z",
     "start_time": "2021-12-27T16:16:52.029806Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_skip = ['customer_id','week_end','min_week','max_week','purchase_flag_next_4w']\n",
    "for cols in df_base.drop(*cols_skip).columns:\n",
    "    df_base = df_base.withColumn(cols+'_4w',  F.sum(F.col(cols)).over(window_4w))\n",
    "    df_base = df_base.withColumn(cols+'_13w', F.sum(F.col(cols)).over(window_13w))\n",
    "    df_base = df_base.withColumn(cols+'_26w', F.sum(F.col(cols)).over(window_26w))\n",
    "    df_base = df_base.withColumn(cols+'_52w', F.sum(F.col(cols)).over(window_52w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Derived features</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:17:12.186836Z",
     "start_time": "2021-12-27T16:17:09.199308Z"
    }
   },
   "outputs": [],
   "source": [
    "# aov, aur, upt at each time cut\n",
    "for cols in ['sales','orders','units']:\n",
    "    for time_cuts in ['1w','_4w','_13w','_26w','_52w']:\n",
    "        if time_cuts=='1w': time_cuts=''\n",
    "        df_base = df_base.withColumn('aov'+time_cuts, F.col('sales'+time_cuts)/F.col('orders'+time_cuts))\n",
    "        df_base = df_base.withColumn('aur'+time_cuts, F.col('sales'+time_cuts)/F.col('units'+time_cuts))\n",
    "        df_base = df_base.withColumn('upt'+time_cuts, F.col('units'+time_cuts)/F.col('orders'+time_cuts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:17:18.265086Z",
     "start_time": "2021-12-27T16:17:17.471920Z"
    }
   },
   "outputs": [],
   "source": [
    "# % split of category and payment type for 26w (can be extended to other timeframes as well)\n",
    "for cat in ['A','B','C','D','E']:\n",
    "    df_base = df_base.withColumn('cat_'+cat+'_26w_perc', F.col('cat_'+cat+'_26w')/F.col('sales_26w'))\n",
    "\n",
    "for pay in ['cash', 'credit_card', 'debit_card', 'gift_card', 'others']:\n",
    "    df_base = df_base.withColumn('pay_'+pay+'_26w_perc', F.col('pay_'+pay+'_26w')/F.col('sales_26w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:17:20.680698Z",
     "start_time": "2021-12-27T16:17:20.657898Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_id',\n",
       " 'week_end',\n",
       " 'min_week',\n",
       " 'max_week',\n",
       " 'sales',\n",
       " 'orders',\n",
       " 'units',\n",
       " 'cat_A',\n",
       " 'cat_B',\n",
       " 'cat_C',\n",
       " 'cat_D',\n",
       " 'cat_E',\n",
       " 'pay_cash',\n",
       " 'pay_credit_card',\n",
       " 'pay_debit_card',\n",
       " 'pay_gift_card',\n",
       " 'pay_others',\n",
       " 'purchase_flag',\n",
       " 'purchase_flag_next_4w',\n",
       " 'sales_4w',\n",
       " 'sales_13w',\n",
       " 'sales_26w',\n",
       " 'sales_52w',\n",
       " 'orders_4w',\n",
       " 'orders_13w',\n",
       " 'orders_26w',\n",
       " 'orders_52w',\n",
       " 'units_4w',\n",
       " 'units_13w',\n",
       " 'units_26w',\n",
       " 'units_52w',\n",
       " 'cat_A_4w',\n",
       " 'cat_A_13w',\n",
       " 'cat_A_26w',\n",
       " 'cat_A_52w',\n",
       " 'cat_B_4w',\n",
       " 'cat_B_13w',\n",
       " 'cat_B_26w',\n",
       " 'cat_B_52w',\n",
       " 'cat_C_4w',\n",
       " 'cat_C_13w',\n",
       " 'cat_C_26w',\n",
       " 'cat_C_52w',\n",
       " 'cat_D_4w',\n",
       " 'cat_D_13w',\n",
       " 'cat_D_26w',\n",
       " 'cat_D_52w',\n",
       " 'cat_E_4w',\n",
       " 'cat_E_13w',\n",
       " 'cat_E_26w',\n",
       " 'cat_E_52w',\n",
       " 'pay_cash_4w',\n",
       " 'pay_cash_13w',\n",
       " 'pay_cash_26w',\n",
       " 'pay_cash_52w',\n",
       " 'pay_credit_card_4w',\n",
       " 'pay_credit_card_13w',\n",
       " 'pay_credit_card_26w',\n",
       " 'pay_credit_card_52w',\n",
       " 'pay_debit_card_4w',\n",
       " 'pay_debit_card_13w',\n",
       " 'pay_debit_card_26w',\n",
       " 'pay_debit_card_52w',\n",
       " 'pay_gift_card_4w',\n",
       " 'pay_gift_card_13w',\n",
       " 'pay_gift_card_26w',\n",
       " 'pay_gift_card_52w',\n",
       " 'pay_others_4w',\n",
       " 'pay_others_13w',\n",
       " 'pay_others_26w',\n",
       " 'pay_others_52w',\n",
       " 'purchase_flag_4w',\n",
       " 'purchase_flag_13w',\n",
       " 'purchase_flag_26w',\n",
       " 'purchase_flag_52w',\n",
       " 'aov',\n",
       " 'aur',\n",
       " 'upt',\n",
       " 'aov_4w',\n",
       " 'aur_4w',\n",
       " 'upt_4w',\n",
       " 'aov_13w',\n",
       " 'aur_13w',\n",
       " 'upt_13w',\n",
       " 'aov_26w',\n",
       " 'aur_26w',\n",
       " 'upt_26w',\n",
       " 'aov_52w',\n",
       " 'aur_52w',\n",
       " 'upt_52w',\n",
       " 'cat_A_26w_perc',\n",
       " 'cat_B_26w_perc',\n",
       " 'cat_C_26w_perc',\n",
       " 'cat_D_26w_perc',\n",
       " 'cat_E_26w_perc',\n",
       " 'pay_cash_26w_perc',\n",
       " 'pay_credit_card_26w_perc',\n",
       " 'pay_debit_card_26w_perc',\n",
       " 'pay_gift_card_26w_perc',\n",
       " 'pay_others_26w_perc']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all columns\n",
    "df_base.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Derived features: trend vars</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:18:29.211270Z",
     "start_time": "2021-12-27T16:18:28.949187Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will take ratios sales for different timeframes to estimate trend features\n",
    "# that depict whether a customer has an increasing trend or not\n",
    "df_base = df_base.withColumn('sales_1w_over_4w',   F.col('sales')/    F.col('sales_4w'))\n",
    "df_base = df_base.withColumn('sales_4w_over_13w',  F.col('sales_4w')/ F.col('sales_13w'))\n",
    "df_base = df_base.withColumn('sales_13w_over_26w', F.col('sales_13w')/F.col('sales_26w'))\n",
    "df_base = df_base.withColumn('sales_26w_over_52w', F.col('sales_26w')/F.col('sales_52w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>More derived features</b>:<br/>\n",
    "We can add many more derived features as well, as required.\n",
    "\n",
    "e.g. lag variables of existing features, trend ratios for other features, % change (Q-o-Q, M-o-M type) using lag variales, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:18:39.702492Z",
     "start_time": "2021-12-27T16:18:33.632585Z"
    }
   },
   "outputs": [],
   "source": [
    "# sample rows to csv for checks\n",
    "df_base.limit(10).toPandas().to_csv('./data/rw_features_qc.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:20:58.773341Z",
     "start_time": "2021-12-27T16:20:36.750504Z"
    }
   },
   "outputs": [],
   "source": [
    "# save features dataset as parquet\n",
    "df_base.repartition(8).write.parquet('./data/rw_features/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T16:21:07.619957Z",
     "start_time": "2021-12-27T16:21:07.408777Z"
    }
   },
   "outputs": [],
   "source": [
    "df_features = spark.read.parquet('./data/rw_features/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "368px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
