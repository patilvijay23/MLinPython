{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Using XGBoost with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following notebook showcases an example for using XGBoost with PySpark.\n",
    "\n",
    "XGBoost does not provide a PySpark API in Spark, it only provides Scala and other APIs. Hence we will be using a custom python wrapper for XGBoost built by [sllynn here](https://github.com/sllynn/spark-xgboost/tree/master/sparkxgb).\n",
    "\n",
    "We will be using Spark 2.4.5 with XGBoost 0.9 as it is one the working version pairs.\n",
    "\n",
    "This uses dummy sales data.\n",
    "\n",
    "***\n",
    "\n",
    "<b>Spark 2.4.5</b> (with Python 3.7) has been used for this notebook.<br>\n",
    "Refer to [spark documentation](https://spark.apache.org/docs/2.4.5/api/sql/index.html) for help with <b>data ops functions</b>.<br>\n",
    "Refer to [this article](https://medium.com/analytics-vidhya/installing-and-using-pyspark-on-windows-machine-59c2d64af76e) to <b>install and use PySpark on Windows machine</b> and [this article](https://patilvijay23.medium.com/installing-and-using-pyspark-on-linux-machine-e9f8dddc0c9a) to <b>install and use PySpark on Linux machine</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the required files\n",
    "XGBoost 0.9 Jar files:\n",
    "- `wget https://repo1.maven.org/maven2/ml/dmlc/xgboost4j/0.90/xgboost4j-0.90.jar`\n",
    "- `wget https://repo1.maven.org/maven2/ml/dmlc/xgboost4j-spark/0.90/xgboost4j-spark-0.90.jar`\n",
    "\n",
    "OR\n",
    "\n",
    "- `curl -o xgboost4j-0.90.jar https://repo1.maven.org/maven2/ml/dmlc/xgboost4j/0.90/xgboost4j-0.90.jar`\n",
    "- `curl -o xgboost4j-spark-0.90.jar https://repo1.maven.org/maven2/ml/dmlc/xgboost4j-spark/0.90/xgboost4j-spark-0.90.jar`\n",
    "\n",
    "Wrapper code\n",
    "- `git clone https://github.com/sllynn/spark-xgboost.git`\n",
    "\n",
    "Setup:\n",
    "- Place `sparkxgb` folder from the git repo into your working directory.\n",
    "- Copy the two jar files into `sparkxgb` directory to have everything in one place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a spark session\n",
    "To create a SparkSession, use the following builder pattern:\n",
    " \n",
    "`spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"Word Count\")\\\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "    .getOrCreate()`\n",
    "\n",
    "We will use `.config(\"spark.jars\", \"/path/jar1.jar,/path/jar2.jar\")` to add the required XGBoost jars to the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T06:25:21.626967Z",
     "start_time": "2021-12-28T06:25:21.318113Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T06:25:28.141614Z",
     "start_time": "2021-12-28T06:25:27.634779Z"
    }
   },
   "outputs": [],
   "source": [
    "#initiating spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T06:25:32.603308Z",
     "start_time": "2021-12-28T06:25:31.992692Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"xgboost\")\\\n",
    "    .config(\"spark.executor.memory\", \"1536m\")\\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.jars\", \"sparkxgb/xgboost4j-0.90.jar,sparkxgb/xgboost4j-spark-0.90.jar\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T06:25:34.292160Z",
     "start_time": "2021-12-28T06:25:34.252288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://lenovo-pc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>xgboost</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1db73c33708>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import the wrapper</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkxgb import XGBoostClassifier, XGBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep\n",
    "\n",
    "We will use the model training dataset created in the Rolling Window features notebook- 3_rolling_window_features.\n",
    "\n",
    "The dataset has two y variables, one for each classification and regression tasks. We will use those to build classification and regression models using XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T06:26:58.796270Z",
     "start_time": "2021-12-28T06:26:56.691183Z"
    }
   },
   "outputs": [],
   "source": [
    "df_features = spark.read.parquet('./data/rw_features/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sample one week_end per month</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T06:38:21.043453Z",
     "start_time": "2021-12-28T06:38:17.903470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wk_sample = df_features.select('week_end').withColumn('month', F.substring(F.col('week_end'), 1,7))\n",
    "df_wk_sample = df_wk_sample.groupBy('month').agg(F.max('week_end').alias('week_end'))\n",
    "\n",
    "df_wk_sample = df_wk_sample.repartition(1).persist()\n",
    "df_wk_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T06:38:27.930926Z",
     "start_time": "2021-12-28T06:38:27.595969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|  month|  week_end|\n",
      "+-------+----------+\n",
      "|2019-01|2019-01-26|\n",
      "|2019-02|2019-02-23|\n",
      "|2019-03|2019-03-30|\n",
      "|2019-04|2019-04-27|\n",
      "|2019-05|2019-05-25|\n",
      "+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_wk_sample.sort('week_end').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T06:38:41.350445Z",
     "start_time": "2021-12-28T06:38:40.057886Z"
    }
   },
   "outputs": [],
   "source": [
    "# join back to filer\n",
    "df_model = df_features.join(F.broadcast(df_wk_sample.select('week_end')), on=['week_end'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Eligibility filter</b>: Customer should be active in last year w.r.t the reference date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T06:38:58.040892Z",
     "start_time": "2021-12-28T06:38:56.899447Z"
    }
   },
   "outputs": [],
   "source": [
    "# use sales_52w for elig. filter\n",
    "df_model = df_model.where(F.col('sales_52w')>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Removing latest 4 week_end dates</b>: As we have a look-forward period of 4 weeks, latest 4 week_end dates in the data cannot be used for our model as these do not have 4 weeks ahead of them for the y-variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T06:39:39.533989Z",
     "start_time": "2021-12-28T06:39:36.342179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  week_end|\n",
      "+----------+\n",
      "|2020-12-05|\n",
      "|2020-11-28|\n",
      "|2020-11-21|\n",
      "|2020-11-14|\n",
      "|2020-11-07|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see latest week_end dates (in the dataframe prior to monthly sampling)\n",
    "df_features.select('week_end').drop_duplicates().sort(F.col('week_end').desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T06:39:43.575213Z",
     "start_time": "2021-12-28T06:39:42.379482Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter\n",
    "df_model = df_model.where(F.col('week_end')<'2020-11-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna\n",
    "df_model = df_model.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_model.randomSplit([0.8, 0.2], seed=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['week_end',\n",
       " 'customer_id',\n",
       " 'min_week',\n",
       " 'max_week',\n",
       " 'sales',\n",
       " 'orders',\n",
       " 'units',\n",
       " 'cat_A',\n",
       " 'cat_B',\n",
       " 'cat_C',\n",
       " 'cat_D',\n",
       " 'cat_E',\n",
       " 'pay_cash',\n",
       " 'pay_credit_card',\n",
       " 'pay_debit_card',\n",
       " 'pay_gift_card',\n",
       " 'pay_others',\n",
       " 'purchase_flag',\n",
       " 'purchase_flag_next_4w',\n",
       " 'sales_next_4w',\n",
       " 'sales_4w',\n",
       " 'sales_13w',\n",
       " 'sales_26w',\n",
       " 'sales_52w',\n",
       " 'orders_4w',\n",
       " 'orders_13w',\n",
       " 'orders_26w',\n",
       " 'orders_52w',\n",
       " 'units_4w',\n",
       " 'units_13w',\n",
       " 'units_26w',\n",
       " 'units_52w',\n",
       " 'cat_A_4w',\n",
       " 'cat_A_13w',\n",
       " 'cat_A_26w',\n",
       " 'cat_A_52w',\n",
       " 'cat_B_4w',\n",
       " 'cat_B_13w',\n",
       " 'cat_B_26w',\n",
       " 'cat_B_52w',\n",
       " 'cat_C_4w',\n",
       " 'cat_C_13w',\n",
       " 'cat_C_26w',\n",
       " 'cat_C_52w',\n",
       " 'cat_D_4w',\n",
       " 'cat_D_13w',\n",
       " 'cat_D_26w',\n",
       " 'cat_D_52w',\n",
       " 'cat_E_4w',\n",
       " 'cat_E_13w',\n",
       " 'cat_E_26w',\n",
       " 'cat_E_52w',\n",
       " 'pay_cash_4w',\n",
       " 'pay_cash_13w',\n",
       " 'pay_cash_26w',\n",
       " 'pay_cash_52w',\n",
       " 'pay_credit_card_4w',\n",
       " 'pay_credit_card_13w',\n",
       " 'pay_credit_card_26w',\n",
       " 'pay_credit_card_52w',\n",
       " 'pay_debit_card_4w',\n",
       " 'pay_debit_card_13w',\n",
       " 'pay_debit_card_26w',\n",
       " 'pay_debit_card_52w',\n",
       " 'pay_gift_card_4w',\n",
       " 'pay_gift_card_13w',\n",
       " 'pay_gift_card_26w',\n",
       " 'pay_gift_card_52w',\n",
       " 'pay_others_4w',\n",
       " 'pay_others_13w',\n",
       " 'pay_others_26w',\n",
       " 'pay_others_52w',\n",
       " 'purchase_flag_4w',\n",
       " 'purchase_flag_13w',\n",
       " 'purchase_flag_26w',\n",
       " 'purchase_flag_52w',\n",
       " 'aov',\n",
       " 'aur',\n",
       " 'upt',\n",
       " 'aov_4w',\n",
       " 'aur_4w',\n",
       " 'upt_4w',\n",
       " 'aov_13w',\n",
       " 'aur_13w',\n",
       " 'upt_13w',\n",
       " 'aov_26w',\n",
       " 'aur_26w',\n",
       " 'upt_26w',\n",
       " 'aov_52w',\n",
       " 'aur_52w',\n",
       " 'upt_52w',\n",
       " 'cat_A_26w_perc',\n",
       " 'cat_B_26w_perc',\n",
       " 'cat_C_26w_perc',\n",
       " 'cat_D_26w_perc',\n",
       " 'cat_E_26w_perc',\n",
       " 'pay_cash_26w_perc',\n",
       " 'pay_credit_card_26w_perc',\n",
       " 'pay_debit_card_26w_perc',\n",
       " 'pay_gift_card_26w_perc',\n",
       " 'pay_others_26w_perc',\n",
       " 'sales_1w_over_4w',\n",
       " 'sales_4w_over_13w',\n",
       " 'sales_13w_over_26w',\n",
       " 'sales_26w_over_52w',\n",
       " 'year',\n",
       " 'month',\n",
       " 'weekofyear']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Dataset Summary\n",
    "Let's look at event rate for our dataset and also get a quick summary of all features.\n",
    "\n",
    "The y-variable is balanced here because it is a dummy dataset. <mark>In most actual scenarios, this will not be balanced and the model build exercise will involving sampling for balancing.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|purchase_flag_next_4w|count|\n",
      "+---------------------+-----+\n",
      "|                    0| 8328|\n",
      "|                    1| 8312|\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupBy('purchase_flag_next_4w').count().sort('purchase_flag_next_4w').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|purchase_flag_next_4w|count|\n",
      "+---------------------+-----+\n",
      "|                    0| 2136|\n",
      "|                    1| 2162|\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.groupBy('purchase_flag_next_4w').count().sort('purchase_flag_next_4w').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing pipeline\n",
    "The below pipeline only does pre-processing and saves it to be used for scoring.\n",
    "\n",
    "You can also add the model step to this to have a single pipeline instead of two that I have created. Though having two pipelines makes it easier to iterate through just the model step during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features: remove identifier columns and the y-var\n",
    "col_list = df_model.drop('week_end','customer_id','min_week','max_week','purchase_flag_next_4w','sales_next_4w').columns\n",
    "\n",
    "stages = []\n",
    "assembler = VectorAssembler(inputCols=col_list, outputCol='features')\n",
    "stages.append(assembler)\n",
    "\n",
    "pipe = Pipeline(stages=stages)\n",
    "pipe_model = pipe.fit(train)\n",
    "\n",
    "pipe_model.write().overwrite().save('./files/model_objects/xgb_clf_pipe/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_model = PipelineModel.load('./files/model_objects/xgb_clf_pipe/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Apply the transformation pipeline</b>\n",
    "\n",
    "Also keep the identifier columns and y-var in the transformed dataframe.\n",
    "\n",
    "<mark>We are keeping both the classification and regression y-vars here as we will be re-using the same processed dataset for the regression section.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16640"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pr = pipe_model.transform(train)\n",
    "train_pr = train_pr.select('customer_id','week_end','purchase_flag_next_4w','sales_next_4w','features')\n",
    "train_pr = train_pr.persist()\n",
    "train_pr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4298"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pr = pipe_model.transform(test)\n",
    "test_pr = test_pr.select('customer_id','week_end','purchase_flag_next_4w','sales_next_4w','features')\n",
    "test_pr = test_pr.persist()\n",
    "test_pr.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "We will train one iteration of XGBoost classification model as showcase.\n",
    "\n",
    "In actual scenario, you will have to iterate through the training step multiple times for feature selection and model hyper parameter tuning to get a good final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------------------+-------------+--------------------+\n",
      "|customer_id|  week_end|purchase_flag_next_4w|sales_next_4w|            features|\n",
      "+-----------+----------+---------------------+-------------+--------------------+\n",
      "|         27|2019-01-26|                    1|          120|(102,[14,15,16,17...|\n",
      "|         29|2019-01-26|                    0|            0|(102,[0,1,2,3,10,...|\n",
      "|         40|2019-01-26|                    1|         1705|(102,[0,1,2,3,9,1...|\n",
      "|         69|2019-01-26|                    1|         1100|(102,[14,15,16,17...|\n",
      "|         70|2019-01-26|                    1|          400|(102,[0,1,2,3,10,...|\n",
      "+-----------+----------+---------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model requires a PipeLine object for the save and load steps to work properly\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d640c79d7289>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msubsample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mnumRound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     numWorkers=1)\n\u001b[0m",
      "\u001b[1;32m~\\spark_setup\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Docs\\Work\\github\\MLinPython\\pyspark\\sparkxgb\\xgboost.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, alpha, baseMarginCol, baseScore, cacheTrainingSet, checkpointInterval, checkpointPath, colsampleBylevel, colsampleBytree, contribPredictionCol, eta, evalMetric, featuresCol, gamma, growPolicy, interactionConstraints, labelCol, lambda_, lambdaBias, leafPredictionCol, maxBins, maxDeltaStep, maxDepth, maxLeaves, maximizeEvaluationMetrics, minChildWeight, missing, monotoneConstraints, normalizeType, nthread, numClass, numEarlyStoppingRounds, numRound, numWorkers, objective, objectiveType, predictionCol, probabilityCol, rateDrop, rawPredictionCol, sampleType, scalePosWeight, seed, silent, sketchEps, skipDrop, subsample, thresholds, timeoutRequestWorkers, trainTestRatio, treeLimit, treeMethod, useExternalMemory, verbosity, weightCol)\u001b[0m\n\u001b[0;32m     83\u001b[0m                  \u001b[0mverbosity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                  weightCol=None):\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXGBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"lambda_\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Docs\\Work\\github\\MLinPython\\pyspark\\sparkxgb\\common.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, classname)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXGboostEstimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_class_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_params_from_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_param_getters_and_setters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark_setup\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[1;34m(java_class, *args)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mjava_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "xgboost=XGBoostClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"purchase_flag_next_4w\",\n",
    "    predictionCol=\"prediction\",\n",
    "    objective=\"binary:logistic\", \n",
    "    evalMetric=\"logloss\",\n",
    "    maxDepth=15,\n",
    "    missing=0.0,\n",
    "    subsample=0.7,\n",
    "    numRound=50,\n",
    "    numWorkers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages.append(xgboost)\n",
    "pipe = Pipeline(stages=stages)\n",
    "\n",
    "start_time = time()\n",
    "# model = xgboost.fit(train_pr)\n",
    "model = pipe.fit(train_pr)\n",
    "print('time elapsed: ', np.round(time()-start_time,2),'s',sep='')\n",
    "\n",
    "# model.write().overwrite().save('s3://marketing-analytics-pal/dev/adhoc/vijay/xgb_classifier_09/')\n",
    "model.write().overwrite().save('s3://marketing-analytics-pal/dev/adhoc/vijay/xgb_classifier_09/')\n",
    "\n",
    "# model = XGBoostClassificationModel.load(path='s3://marketing-analytics-pal/dev/adhoc/vijay/xgb_classifier_09/')\n",
    "model = PipelineModel.load(path='s3://marketing-analytics-pal/dev/adhoc/vijay/xgb_classifier_09/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'labelCol': 'purchase_flag_next_4w',\n",
    "    'numTrees': 128, # default: 128\n",
    "    'maxDepth': 12,  # default: 12\n",
    "    'featuresCol': 'features',\n",
    "    'minInstancesPerNode': 25,\n",
    "    'maxBins': 128,\n",
    "    'minInfoGain': 0.0,\n",
    "    'subsamplingRate': 0.7,\n",
    "    'featureSubsetStrategy': '0.3',\n",
    "    'impurity': 'gini',\n",
    "    'seed': 125,\n",
    "    'cacheNodeIds': False,\n",
    "    'maxMemoryInMB': 256\n",
    "    }\n",
    "\n",
    "clf = RandomForestClassifier(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_clf = clf.fit(train_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "We will save feature importance as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance_list = trained_clf.featureImportances\n",
    "feature_list = pd.DataFrame(train_pr.schema['features'].metadata['ml_attr']['attrs']['numeric']).sort_values('idx')\n",
    "\n",
    "feature_importance_list = pd.DataFrame(\n",
    "    data=feature_importance_list.toArray(),\n",
    "    columns=['relative_importance'],\n",
    "    index=feature_list['name'])\n",
    "feature_importance_list = feature_importance_list.sort_values('relative_importance', ascending=False)\n",
    "\n",
    "feature_importance_list.to_csv('./files/rw_rf_feat_imp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondelement = F.udf(lambda v: float(v[1]), FloatType())\n",
    "\n",
    "train_pred = trained_clf.transform(train_pr).withColumn('score',secondelement(F.col('probability')))\n",
    "test_pred =  trained_clf.transform(test_pr).withColumn('score', secondelement(F.col('probability')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------------------+--------------------+--------------------+--------------------+----------+----------+\n",
      "|customer_id|  week_end|purchase_flag_next_4w|            features|       rawPrediction|         probability|prediction|     score|\n",
      "+-----------+----------+---------------------+--------------------+--------------------+--------------------+----------+----------+\n",
      "|         15|2019-01-26|                    1|(102,[0,1,2,6,9,1...|[53.0784863119179...|[0.41467567431185...|       1.0|0.58532435|\n",
      "|         27|2019-01-26|                    1|(102,[14,15,16,17...|[51.1393971441854...|[0.39952654018894...|       1.0|0.60047346|\n",
      "|         28|2019-01-26|                    1|(102,[14,15,16,17...|[49.8725279055357...|[0.38962912426199...|       1.0| 0.6103709|\n",
      "|        170|2019-01-26|                    0|(102,[0,1,2,7,10,...|[54.1789500442362...|[0.42327304722059...|       1.0|  0.576727|\n",
      "|        192|2019-01-26|                    0|(102,[0,1,2,7,10,...|[57.5312187445332...|[0.44946264644166...|       1.0|0.55053735|\n",
      "+-----------+----------+---------------------+--------------------+--------------------+--------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(\n",
    "        rawPredictionCol='rawPrediction',\n",
    "        labelCol='purchase_flag_next_4w',\n",
    "        metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8116811886255015"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# areaUnderROC\n",
    "evaluator.evaluate(train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7412597272276923"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------+-----+\n",
      "|purchase_flag_next_4w|prediction|count|\n",
      "+---------------------+----------+-----+\n",
      "|                    0|       0.0| 1655|\n",
      "|                    0|       1.0|  450|\n",
      "|                    1|       0.0|  937|\n",
      "|                    1|       1.0| 1120|\n",
      "+---------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cm\n",
    "test_pred.groupBy('purchase_flag_next_4w','prediction').count().sort('purchase_flag_next_4w','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6667467563671312"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "test_pred.where(F.col('purchase_flag_next_4w')==F.col('prediction')).count()/test_pred.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_clf.write().overwrite().save('./files/model_objects/rw_rf_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_clf = RandomForestClassificationModel.load('./files/model_objects/rw_rf_model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "We will take the records for latest week_end from df_features and score it using our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = spark.read.parquet('./data/rw_features/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2020, 12, 5)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_we = df_features.selectExpr('max(week_end)').collect()[0][0]\n",
    "max_we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scoring = df_features.where(F.col('week_end')==max_we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scoring.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna\n",
    "df_scoring = df_scoring.fillna(0)\n",
    "\n",
    "# transformation pipeline\n",
    "pipe_model = PipelineModel.load('./files/model_objects/rw_pipe/')\n",
    "\n",
    "# apply\n",
    "df_scoring = pipe_model.transform(df_scoring)\n",
    "df_scoring = df_scoring.select('customer_id','week_end','features')\n",
    "\n",
    "# rf model\n",
    "trained_clf = RandomForestClassificationModel.load('./files/model_objects/rw_rf_model/')\n",
    "\n",
    "#apply\n",
    "secondelement = F.udf(lambda v: float(v[1]), FloatType())\n",
    "\n",
    "df_scoring = trained_clf.transform(df_scoring).withColumn('score',secondelement(F.col('probability')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+--------------------+--------------------+----------+----------+\n",
      "|customer_id|  week_end|            features|       rawPrediction|         probability|prediction|     score|\n",
      "+-----------+----------+--------------------+--------------------+--------------------+----------+----------+\n",
      "|        148|2020-12-05|[1200.0,1.0,3.0,1...|[68.4381452599104...|[0.53467300984305...|       0.0|  0.465327|\n",
      "|        787|2020-12-05|(102,[15,16,17,19...|[96.4135634068059...|[0.75323096411567...|       0.0|0.24676904|\n",
      "|        906|2020-12-05|(102,[16,17,20,21...|[90.4284145792717...|[0.70647198890056...|       0.0|0.29352802|\n",
      "|        182|2020-12-05|(102,[16,17,20,21...|[90.9264612678324...|[0.71036297865494...|       0.0|0.28963703|\n",
      "|        442|2020-12-05|(102,[16,17,20,21...|[105.838704191258...|[0.82686487649421...|       0.0|0.17313512|\n",
      "+-----------+----------+--------------------+--------------------+--------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scoring.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scored output\n",
    "df_scoring.repartition(8).write.parquet('./data/rw_scored/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "368px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
