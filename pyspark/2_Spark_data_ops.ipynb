{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Spark Data Operations (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following notebook showcases code snippets for the frequently used data operations on Spark dataframes.\n",
    "\n",
    "It covers following ops:\n",
    "- Dataframes\n",
    " - Loading data\n",
    "- Basic ops: shape, select columns, show data sample\n",
    "- Data info\n",
    " - Schema\n",
    " - Summary\n",
    "- Filtering Data\n",
    "- Add new columns\n",
    "- Joins- both with same name and with different names\n",
    "- Aggregations\n",
    "- Sorting\n",
    "- Pivot- single and multiple and the difference\n",
    "- Window functions\n",
    "- EDA related steps: fillna, etc.\n",
    "- Writing SQL codes- quick run through of all above things via sql, small examples\n",
    "- Persist; lazy eval; actions; SparkUI to look at storage\n",
    "- Partitioning- getting number of partitions of existing dfs, repartition options\n",
    "- Writing data; file types; disk partitioning while writing\n",
    "\n",
    "***\n",
    "\n",
    "<b>Spark 3.1.2</b> (with Python 3.8) has been used for this notebook.<br>\n",
    "Refer to [spark documentation](https://spark.apache.org/docs/3.1.2/api/sql/index.html) for help with <b>data ops functions</b>.<br>\n",
    "Refer to [this article](https://medium.com/analytics-vidhya/installing-and-using-pyspark-on-windows-machine-59c2d64af76e) to <b>install and use PySpark on Windows machine</b>.\n",
    "\n",
    "***\n",
    "\n",
    "<mark><b>Note</b></mark>: We are dealing with a sample dataset in this exercise and hence I have freely used `.show()`, `.count()`, `.collect()` actions on the dataframes. Please be careful with such steps on actual datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a spark session\n",
    "To create a SparkSession, use the following builder pattern:\n",
    " \n",
    "`spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"Word Count\")\\\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "    .getOrCreate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiating spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"data_ops\")\\\n",
    "    .config(\"spark.executor.memory\", \"2700m\")\\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://lenovo-pc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://127.0.0.1:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data_ops</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x18bf682c850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes\n",
    "\n",
    "A DataFrame is a Dataset organized into named columns.<br>\n",
    "It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.<br>DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv files\n",
    "\n",
    "Usually, first line of a csv file is the header and we want to infer the schema of the file.<br>\n",
    "So below command generally suffices for most use cases.\n",
    "\n",
    "`spark.read.csv(path, inferSchema=True, header=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Parameters: [reference](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html)\n",
    "1. `path` : str or list<br>\n",
    "path to csv location. String, or list of strings, for input path(s), or RDD of Strings storing CSV rows. If this points to a directory, all files under that directory are read and appended.\n",
    "\n",
    "2. `inferSchema` : str or bool, optional<br>\n",
    "infers the input schema automatically from data. It requires one extra pass over the data. If None is set, it uses the default value, false.<br>\n",
    "Or you can specify the input schema using `schema` param\n",
    "\n",
    "3. `schema` : pyspark.sql.types.StructType or str, optional<br>\n",
    "an optional pyspark.sql.types.StructType for the input schema or a DDL-formatted string (For example col0 INT, col1 DOUBLE).\n",
    "\n",
    "4. `sep` : str, optional<br>\n",
    "sets a separator (one or more characters) for each field and value. If None is set, it uses the default value, ,\n",
    "\n",
    "5. `header` : str or bool, optional<br>\n",
    "uses the first line as names of columns. If None is set, it uses the default value, false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = spark.read.csv('./data/clustering_sales.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet files\n",
    "\n",
    "Parquet is an open source file format available to any project in the Hadoop ecosystem. Apache Parquet is designed for efficient as well as performant flat columnar storage format of data compared to row based files like CSV or TSV files.\n",
    "\n",
    "Parquet files can 1/10th or lesser in size compared to csv format and hence are preferred storage format.\n",
    "\n",
    "`spark.read.parquet(path)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Parameters: [reference](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameReader.parquet.html)\n",
    "\n",
    "1. `paths` : str\n",
    "2. `mergeSchema` : str or bool, optional<br>\n",
    "sets whether we should merge schemas collected from all Parquet part-files. This will override spark.sql.parquet.mergeSchema. The default value is specified in spark.sql.parquet.mergeSchema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = spark.read.parquet('./data/clustering_features/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column count\n",
    "len(df_sales.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row count\n",
    "df_sales.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|order_id|order_item_id|   tran_dt|customer_id|dollars|qty|product_id|payment_type_id|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|       1|            1|2020-01-01|        572|    550|  1|        20|              2|\n",
      "|       2|            2|2020-01-01|        532|    630|  3|        11|              2|\n",
      "|       3|            3|2020-01-01|        608|    450|  2|        18|              4|\n",
      "|       4|            4|2020-01-01|        424|    110|  2|        10|              2|\n",
      "|       5|            5|2020-01-01|        584|    250|  1|         8|              4|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id',\n",
       " 'order_item_id',\n",
       " 'tran_dt',\n",
       " 'customer_id',\n",
       " 'dollars',\n",
       " 'qty',\n",
       " 'product_id',\n",
       " 'payment_type_id']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_sales.select('order_id','customer_id','dollars','product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+----------+\n",
      "|order_id|customer_id|dollars|product_id|\n",
      "+--------+-----------+-------+----------+\n",
      "|       1|        572|    550|        20|\n",
      "|       2|        532|    630|        11|\n",
      "|       3|        608|    450|        18|\n",
      "|       4|        424|    110|        10|\n",
      "|       5|        584|    250|         8|\n",
      "+--------+-----------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_subset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming columns\n",
    "\n",
    "Let's rename 'dollars' column to 'sales': `df.withColumnRenamed('source_column','new_name')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rename = df_sales.withColumnRenamed('dollars','sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id',\n",
       " 'order_item_id',\n",
       " 'tran_dt',\n",
       " 'customer_id',\n",
       " 'sales',\n",
       " 'qty',\n",
       " 'product_id',\n",
       " 'payment_type_id']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rename.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.withColumnRenamed` will NOT throw an error when the source column is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = df_sales.withColumnRenamed('unknown_source_column','new_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bulk rename\n",
    "There are two ways of bulk renaming columns:\n",
    "1. Loop `.withColumnRenamed()` to rename the columns\n",
    "2. use `.toDF()` to rename the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Loop</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a copy of the df for this task\n",
    "df_rename = df_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add _r to all column names\n",
    "for col in df_rename.columns:\n",
    "    df_rename = df_rename.withColumnRenamed(col, col+'_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id_r',\n",
       " 'order_item_id_r',\n",
       " 'tran_dt_r',\n",
       " 'customer_id_r',\n",
       " 'dollars_r',\n",
       " 'qty_r',\n",
       " 'product_id_r',\n",
       " 'payment_type_id_r']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rename.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`.toDF()`</b>\n",
    "\n",
    "This takes a list of new column names and replaces existing column names with the list. The replacement happens in the sequence in which columns are present in the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a copy of the df for this task\n",
    "df_rename = df_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add _r to all column names\n",
    "list_new_names = [x + '_r' for x in df_rename.columns]\n",
    "df_rename = df_rename.toDF(*list_new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id_r',\n",
       " 'order_item_id_r',\n",
       " 'tran_dt_r',\n",
       " 'customer_id_r',\n",
       " 'dollars_r',\n",
       " 'qty_r',\n",
       " 'product_id_r',\n",
       " 'payment_type_id_r']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rename.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- tran_dt: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- dollars: integer (nullable = true)\n",
      " |-- qty: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- payment_type_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema information\n",
    "df_sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+----------+-----------------+-----------------+------------------+-----------------+------------------+\n",
      "|summary|          order_id|     order_item_id|   tran_dt|      customer_id|          dollars|               qty|       product_id|   payment_type_id|\n",
      "+-------+------------------+------------------+----------+-----------------+-----------------+------------------+-----------------+------------------+\n",
      "|  count|             10000|             10000|     10000|            10000|            10000|             10000|            10000|             10000|\n",
      "|   mean|          4907.044|            5000.5|      null|         491.5132|          570.717|            2.1108|           11.494|            2.5942|\n",
      "| stddev|2831.8233199116644|2886.8956799071675|      null|292.5619908286239|421.2652399038348|0.8827237847299767|6.344287903216555|0.8613945437811904|\n",
      "|    min|                 1|                 1|2020-01-01|                1|               50|                 1|                1|                 1|\n",
      "|    25%|              2455|              2499|      null|              235|              240|                 1|                6|                 2|\n",
      "|    50%|              4909|              4999|      null|              476|              450|                 2|               11|                 2|\n",
      "|    75%|              7358|              7499|      null|              748|              800|                 3|               17|                 3|\n",
      "|    max|              9811|             10000|2020-12-01|             1000|             2200|                 4|               22|                 5|\n",
      "+-------+------------------+------------------+----------+-----------------+-----------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data summary\n",
    "df_sales.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting more detailed information from .summary()\n",
    "summary_stats = (\n",
    "    'count','mean','stddev','min','0.10%','1.00%','5.00%','10.00%','20.00%','25.00%','30.00%',\n",
    "    '40.00%','50.00%','60.00%','70.00%','75.00%','80.00%','90.00%','95.00%','99.00%','99.90%','max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+------------------+\n",
      "|summary|tran_dt   |dollars          |qty               |\n",
      "+-------+----------+-----------------+------------------+\n",
      "|count  |10000     |10000            |10000             |\n",
      "|mean   |null      |570.717          |2.1108            |\n",
      "|stddev |null      |421.2652399038348|0.8827237847299767|\n",
      "|min    |2020-01-01|50               |1                 |\n",
      "|0.10%  |null      |50               |1                 |\n",
      "|1.00%  |null      |50               |1                 |\n",
      "|5.00%  |null      |80               |1                 |\n",
      "|10.00% |null      |120              |1                 |\n",
      "|20.00% |null      |210              |1                 |\n",
      "|25.00% |null      |240              |1                 |\n",
      "|30.00% |null      |270              |2                 |\n",
      "|40.00% |null      |400              |2                 |\n",
      "|50.00% |null      |450              |2                 |\n",
      "|60.00% |null      |550              |2                 |\n",
      "|70.00% |null      |750              |3                 |\n",
      "|75.00% |null      |800              |3                 |\n",
      "|80.00% |null      |900              |3                 |\n",
      "|90.00% |null      |1170             |3                 |\n",
      "|95.00% |null      |1350             |4                 |\n",
      "|99.00% |null      |1800             |4                 |\n",
      "|99.90% |null      |2200             |4                 |\n",
      "|max    |2020-12-01|2200             |4                 |\n",
      "+-------+----------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.select('tran_dt','dollars','qty').summary(*summary_stats).show(30,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above summary can also be converted to a Pandas dataframe for better display in Jupyter and to save it as csv/excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = df_sales.summary(*summary_stats).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>tran_dt</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>dollars</th>\n",
       "      <th>qty</th>\n",
       "      <th>product_id</th>\n",
       "      <th>payment_type_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>4907.044</td>\n",
       "      <td>5000.5</td>\n",
       "      <td>None</td>\n",
       "      <td>491.5132</td>\n",
       "      <td>570.717</td>\n",
       "      <td>2.1108</td>\n",
       "      <td>11.494</td>\n",
       "      <td>2.5942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>2831.8233199116644</td>\n",
       "      <td>2886.8956799071675</td>\n",
       "      <td>None</td>\n",
       "      <td>292.5619908286239</td>\n",
       "      <td>421.2652399038348</td>\n",
       "      <td>0.8827237847299767</td>\n",
       "      <td>6.344287903216555</td>\n",
       "      <td>0.8613945437811904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.10%</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary            order_id       order_item_id     tran_dt  \\\n",
       "0   count               10000               10000       10000   \n",
       "1    mean            4907.044              5000.5        None   \n",
       "2  stddev  2831.8233199116644  2886.8956799071675        None   \n",
       "3     min                   1                   1  2020-01-01   \n",
       "4   0.10%                   9                   9        None   \n",
       "\n",
       "         customer_id            dollars                 qty  \\\n",
       "0              10000              10000               10000   \n",
       "1           491.5132            570.717              2.1108   \n",
       "2  292.5619908286239  421.2652399038348  0.8827237847299767   \n",
       "3                  1                 50                   1   \n",
       "4                  2                 50                   1   \n",
       "\n",
       "          product_id     payment_type_id  \n",
       "0              10000               10000  \n",
       "1             11.494              2.5942  \n",
       "2  6.344287903216555  0.8613945437811904  \n",
       "3                  1                   1  \n",
       "4                  1                   1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.to_csv('./files/df_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data\n",
    "1. Using `.where()`\n",
    "2. Using `.filter()`\n",
    "\n",
    "Both give same results.\n",
    "\n",
    "`.filter()` is the standard Scala name for the function, and `.where()` is for people who prefer SQL-type syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|order_id|order_item_id|   tran_dt|customer_id|dollars|qty|product_id|payment_type_id|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|     390|          400|2020-01-14|        119|   2200|  4|        22|              2|\n",
      "|     586|          601|2020-01-21|        659|   2200|  4|        22|              4|\n",
      "|     775|          792|2020-01-27|        526|   2200|  4|        20|              4|\n",
      "|     956|          976|2020-02-01|        798|   2200|  4|        22|              3|\n",
      "|    1014|         1036|2020-02-03|        495|   2200|  4|        20|              2|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.filter(\"dollars>2000\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|order_id|order_item_id|   tran_dt|customer_id|dollars|qty|product_id|payment_type_id|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|     390|          400|2020-01-14|        119|   2200|  4|        22|              2|\n",
      "|     586|          601|2020-01-21|        659|   2200|  4|        22|              4|\n",
      "|     775|          792|2020-01-27|        526|   2200|  4|        20|              4|\n",
      "|     956|          976|2020-02-01|        798|   2200|  4|        22|              3|\n",
      "|    1014|         1036|2020-02-03|        495|   2200|  4|        20|              2|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.filter(df_sales['dollars']>2000).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|order_id|order_item_id|   tran_dt|customer_id|dollars|qty|product_id|payment_type_id|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|      33|           33|2020-01-02|        941|   1650|  3|        20|              2|\n",
      "|      58|           58|2020-01-03|        711|   1650|  3|        22|              2|\n",
      "|     102|          102|2020-01-05|        475|   1650|  3|        22|              4|\n",
      "|     151|          151|2020-01-06|        695|   1650|  3|        20|              3|\n",
      "|     251|          257|2020-01-09|        764|   1650|  3|        20|              4|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# please add required parenthesis separating the statements\n",
    "df_sales.filter((df_sales['dollars'] > 1500) & (df_sales['qty'] < 4)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of `F.col()` as column identifier instead of dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|order_id|order_item_id|   tran_dt|customer_id|dollars|qty|product_id|payment_type_id|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|      33|           33|2020-01-02|        941|   1650|  3|        20|              2|\n",
      "|      58|           58|2020-01-03|        711|   1650|  3|        22|              2|\n",
      "|     102|          102|2020-01-05|        475|   1650|  3|        22|              4|\n",
      "|     151|          151|2020-01-06|        695|   1650|  3|        20|              3|\n",
      "|     251|          257|2020-01-09|        764|   1650|  3|        20|              4|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# please add required parenthesis separating the statements\n",
    "df_sales.filter((F.col('dollars') > 1500) & (F.col('qty') < 4)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same output from `.where()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|order_id|order_item_id|   tran_dt|customer_id|dollars|qty|product_id|payment_type_id|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|     390|          400|2020-01-14|        119|   2200|  4|        22|              2|\n",
      "|     586|          601|2020-01-21|        659|   2200|  4|        22|              4|\n",
      "|     775|          792|2020-01-27|        526|   2200|  4|        20|              4|\n",
      "|     956|          976|2020-02-01|        798|   2200|  4|        22|              3|\n",
      "|    1014|         1036|2020-02-03|        495|   2200|  4|        20|              2|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.where(\"dollars>2000\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|order_id|order_item_id|   tran_dt|customer_id|dollars|qty|product_id|payment_type_id|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|     390|          400|2020-01-14|        119|   2200|  4|        22|              2|\n",
      "|     586|          601|2020-01-21|        659|   2200|  4|        22|              4|\n",
      "|     775|          792|2020-01-27|        526|   2200|  4|        20|              4|\n",
      "|     956|          976|2020-02-01|        798|   2200|  4|        22|              3|\n",
      "|    1014|         1036|2020-02-03|        495|   2200|  4|        20|              2|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.where(df_sales['dollars']>2000).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|order_id|order_item_id|   tran_dt|customer_id|dollars|qty|product_id|payment_type_id|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "|     390|          400|2020-01-14|        119|   2200|  4|        22|              2|\n",
      "|     586|          601|2020-01-21|        659|   2200|  4|        22|              4|\n",
      "|     775|          792|2020-01-27|        526|   2200|  4|        20|              4|\n",
      "|     956|          976|2020-02-01|        798|   2200|  4|        22|              3|\n",
      "|    1014|         1036|2020-02-03|        495|   2200|  4|        20|              2|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.where(F.col('dollars')>2000).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new columns\n",
    "`df.withColumn(colName, col)`\n",
    "\n",
    "Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "\n",
    "The column expression must be an expression over this DataFrame; attempting to add a column from some other DataFrame will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = df_sales.withColumn('aur', F.col('dollars')/F.col('qty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+-----+\n",
      "|order_id|order_item_id|   tran_dt|customer_id|dollars|qty|product_id|payment_type_id|  aur|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+-----+\n",
      "|       1|            1|2020-01-01|        572|    550|  1|        20|              2|550.0|\n",
      "|       2|            2|2020-01-01|        532|    630|  3|        11|              2|210.0|\n",
      "|       3|            3|2020-01-01|        608|    450|  2|        18|              4|225.0|\n",
      "|       4|            4|2020-01-01|        424|    110|  2|        10|              2| 55.0|\n",
      "|       5|            5|2020-01-01|        584|    250|  1|         8|              4|250.0|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complicated operations to create new columns: use or case-when\n",
    "df_sales = df_sales.withColumn('payment_2_dollars',F.when(F.col('payment_type_id')==2,F.col('dollars')).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+-----+-----------------+\n",
      "|order_id|order_item_id|   tran_dt|customer_id|dollars|qty|product_id|payment_type_id|  aur|payment_2_dollars|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+-----+-----------------+\n",
      "|       1|            1|2020-01-01|        572|    550|  1|        20|              2|550.0|              550|\n",
      "|       2|            2|2020-01-01|        532|    630|  3|        11|              2|210.0|              630|\n",
      "|       3|            3|2020-01-01|        608|    450|  2|        18|              4|225.0|                0|\n",
      "|       4|            4|2020-01-01|        424|    110|  2|        10|              2| 55.0|              110|\n",
      "|       5|            5|2020-01-01|        584|    250|  1|         8|              4|250.0|                0|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy and Aggregate Functions\n",
    "\n",
    "- GroupBy allows you to group rows together based on some column value, for example, you could group together sales data by the day the sale occured\n",
    "- Once you've performed the GroupBy operation you can use an aggregate functions. An aggregate function aggregates multiple rows of data into a single output, such as taking the sum of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In built Functions\n",
    "\n",
    "There are a variety of functions you can import from pyspark.sql.functions. Check out the documentation for the full list available: \n",
    "\n",
    "Commonly used functions:\n",
    "- sum(): sum the values\n",
    "- avg(): averages the values for the column mentioned\n",
    "- count(): count the number of values in a column (excludes Nulls)\n",
    "- countDistinct(): counts the number of ditinct values in a column (excludes Nulls)\n",
    "- stddev(): finds out the standard deviation of the input values\n",
    "\n",
    "We have imported the `from pyspark.sql.functions` module as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting\n",
    "\n",
    "- Sorting is one of the most common operations applied on dataframes. \n",
    "- It is achieved using the `sort()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+----+-----------------+\n",
      "|order_id|order_item_id|   tran_dt|customer_id|dollars|qty|product_id|payment_type_id| aur|payment_2_dollars|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+----+-----------------+\n",
      "|     544|          559|2020-01-20|        421|     50|  1|         5|              2|50.0|               50|\n",
      "|    1053|         1076|2020-02-04|        262|     50|  1|         5|              3|50.0|                0|\n",
      "|     777|          794|2020-01-27|        404|     50|  1|         5|              3|50.0|                0|\n",
      "|      73|           73|2020-01-03|        198|     50|  1|         5|              3|50.0|                0|\n",
      "|     857|          876|2020-01-29|         61|     50|  1|         5|              3|50.0|                0|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.sort(F.col('dollars')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+-----+-----------------+\n",
      "|order_id|order_item_id|   tran_dt|customer_id|dollars|qty|product_id|payment_type_id|  aur|payment_2_dollars|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+-----+-----------------+\n",
      "|    1014|         1036|2020-02-03|        495|   2200|  4|        20|              2|550.0|             2200|\n",
      "|    2292|         2336|2020-03-17|        506|   2200|  4|        22|              4|550.0|                0|\n",
      "|    1337|         1367|2020-02-13|        955|   2200|  4|        22|              4|550.0|                0|\n",
      "|     956|          976|2020-02-01|        798|   2200|  4|        22|              3|550.0|                0|\n",
      "|    1464|         1496|2020-02-17|        557|   2200|  4|        22|              2|550.0|             2200|\n",
      "+--------+-------------+----------+-----------+-------+---+----------+---------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.sort(F.col('dollars').desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivots\n",
    "You will often find yourself working with Time and Date information, let's walk through some ways you can deal with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA related steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "368px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
